{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, metrics\n",
    "import math\n",
    "def plot_decision_boundary(model, X, y, filename):\n",
    "    \"\"\"\n",
    "    Given a model(a function) and a set of points(X), corresponding labels(y), scatter the points in X with color coding\n",
    "    according to y. Also use the model to predict the label at grid points to get the region for each label, and thus the \n",
    "    descion boundary.\n",
    "    Example usage:\n",
    "    say we have a function predict(x,other params) which makes 0/1 prediction for point x and we want to plot\n",
    "    train set then call as:\n",
    "    plot_decision_boundary(lambda x:predict(x,other params),X_train,Y_train)\n",
    "    params(3): \n",
    "        model : a function which expectes the point to make 0/1 label prediction\n",
    "        X : a (mx2) numpy array with the points\n",
    "        y : a (mx1) numpy array with labels\n",
    "    outputs(None)\n",
    "    \"\"\"\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader(file_obj):\n",
    "    reader = csv.reader(file_obj)\n",
    "    final_ans = []\n",
    "    for row in reader:\n",
    "        temp_ele = []\n",
    "        for ele in row:\n",
    "            temp_ele.append(float(ele))\n",
    "        final_ans.append(temp_ele)\n",
    "    return final_ans\n",
    "\n",
    "def mnist_reader(file_obj):\n",
    "    reader = csv.reader(file_obj)\n",
    "    final_ans = []\n",
    "    final_y = []\n",
    "    for row in reader:\n",
    "#         print(\"Row num\", row[-1])\n",
    "        if(float(row[-1])==6):\n",
    "#             print(\"yeah\")\n",
    "            final_y.append(0.0)\n",
    "        else:\n",
    "            final_y.append(1.0)\n",
    "        temp_ele = []\n",
    "        for ele in row:\n",
    "            temp_ele.append(float(ele))\n",
    "        final_ans.append(temp_ele[:-1])\n",
    "    return final_ans, final_y\n",
    "\n",
    "def csv_reader_y(file_obj):\n",
    "    reader = csv.reader(file_obj)\n",
    "    final_ans = []\n",
    "    for row in reader:\n",
    "        for ele in row:\n",
    "            final_ans.append(float(ele))\n",
    "    return final_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "#     print(\"hello\")\n",
    "#     print(x)\n",
    "    return (1.0/(1.0 + np.exp(-x)))\n",
    "#     print(\"bye\")\n",
    "\n",
    "def fprop(inp, lay_w, lay_b):\n",
    "    outpts = []\n",
    "    outpts.append(np.reshape(inp, [-1]))\n",
    "    for (wei, bia) in zip(lay_w, lay_b):\n",
    "        prop = np.reshape(outpts[-1], [1, -1])\n",
    "        prop = np.add(np.dot(prop, wei),bia)\n",
    "        outpts.append(sigmoid(np.reshape(prop, [-1])))\n",
    "    return outpts\n",
    "\n",
    "def bprop(y_exp, lay_w, lay_b, outpts):\n",
    "    bia_up = []\n",
    "    wei_up = []\n",
    "    last_delta = [-1*(y_exp - outpts[-1][0])]\n",
    "    for (wei, bia, out, out_prev) in zip(reversed(lay_w), reversed(lay_b), reversed(outpts), reversed(outpts[:-1])):\n",
    "        bia_up.append(np.reshape(last_delta, [1, -1]))\n",
    "        out_temp = np.reshape(out_prev, [-1, 1])\n",
    "        total_deriv = np.dot(out_temp, np.reshape(last_delta, [1, -1]))\n",
    "        wei_up.append(total_deriv)\n",
    "        sig_deriv = np.dot(np.diag(out_prev), np.diag(1 - out_prev))\n",
    "        no_oj = np.dot(wei, np.reshape(last_delta, [-1, 1]))\n",
    "        last_delta = np.dot(sig_deriv, no_oj)\n",
    "        last_delta = np.reshape(last_delta, [-1])\n",
    "    return wei_up[::-1], bia_up[::-1]\n",
    "\n",
    "def neural_train(inp, out_exp, hidden_lay, l_rate, batch, max_iter):\n",
    "    wei_up = []\n",
    "    bia_up = []\n",
    "    lay_w = []\n",
    "    lay_b = []\n",
    "    fir = len(inp[0])\n",
    "    for ele in hidden_lay:\n",
    "#         lay_w.append(np.random.rand(fir, ele))\n",
    "        lay_w.append((np.random.rand(fir, ele)-0.5)*10)\n",
    "#         lay_b.append(np.random.rand(1, ele))\n",
    "        lay_b.append((np.random.rand(1, ele)-0.5)*10)\n",
    "        wei_up.append(np.zeros((fir, ele)))\n",
    "        bia_up.append(np.zeros((1, ele)))\n",
    "        fir = ele\n",
    "#     lay_w.append(np.random.rand(fir, 1))\n",
    "    lay_w.append((np.random.rand(fir, 1)-0.5)*10)\n",
    "#     lay_b.append(np.random.rand(1, 1))\n",
    "    lay_b.append((np.random.rand(1, 1)-0.5)*10)\n",
    "    wei_up.append(np.zeros((fir, 1)))\n",
    "    bia_up.append(np.zeros((1, 1)))\n",
    "    err_sum = 0.0\n",
    "    prev_err_sum = 0.0\n",
    "    total_err_sum = 0.0\n",
    "    epoch = 0\n",
    "    while(True):\n",
    "        if(epoch>max_iter):\n",
    "            break\n",
    "        l_rate = 0.001/(math.sqrt(epoch+1))\n",
    "        print(\"Epoch\", epoch)\n",
    "        if(epoch%10==0):\n",
    "            print(epoch, total_err_sum)\n",
    "            total_err_sum = 0.0\n",
    "        for i in range(0, len(inp)):\n",
    "            if(i%1000==0):\n",
    "                print(i)\n",
    "            outputs = fprop(inp[i], lay_w, lay_b)\n",
    "            error = (out_exp[i] - outputs[-1][0])**2\n",
    "            err_sum += error\n",
    "            wup_tmp, bup_tmp = bprop(out_exp[i], lay_w, lay_b, outputs)\n",
    "            for (wei, bia, wup, bup) in zip(wei_up, bia_up, wup_tmp, bup_tmp):\n",
    "                wei += wup\n",
    "                bia += bup\n",
    "            if((i+1)%batch==0):\n",
    "#                 print(wei_up)\n",
    "#                 print(bia_up)\n",
    "                for j in range(len(lay_w)):\n",
    "                    lay_w[j] -= l_rate*wei_up[j]\n",
    "                    lay_b[j] -= l_rate*bia_up[j]\n",
    "                    wei_up[j] = np.zeros(np.shape(wei_up[j]))\n",
    "                    bia_up[j] = np.zeros(np.shape(bia_up[j]))\n",
    "        \n",
    "        epoch += 1\n",
    "        diff = prev_err_sum - err_sum\n",
    "        if(diff < 0.001 and diff >=0):\n",
    "            break\n",
    "        prev_err_sum = err_sum\n",
    "        total_err_sum += err_sum\n",
    "        err_sum = 0.0\n",
    "    return lay_w, lay_b\n",
    "\n",
    "def fprop_new(inp, lay_w, lay_b):\n",
    "    outpts = []\n",
    "    outpts.append(inp)\n",
    "    for (wei, bia) in zip(lay_w, lay_b):\n",
    "        prop = outpts[-1]\n",
    "        prop = np.add(np.dot(prop, wei),bia)\n",
    "        outpts.append(sigmoid(prop))\n",
    "    return outpts\n",
    "\n",
    "\n",
    "def neural_predict(inp):\n",
    "    global lw, lb\n",
    "    outputs = fprop_new(inp, lw, lb)\n",
    "#     print(np.shape(outputs[-1]))\n",
    "    arr = []\n",
    "    for i in range(len(outputs[-1])):\n",
    "        if(outputs[-1][i][0]>=0.5):\n",
    "            arr.append([1])\n",
    "        else:\n",
    "            arr.append([0])\n",
    "    return np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"toy_data/toy_trainX.csv\", \"r\") as f_obj:\n",
    "    train_inp = csv_reader(f_obj)\n",
    "with open(\"toy_data/toy_trainY.csv\", \"r\") as f_obj:\n",
    "    train_out = csv_reader_y(f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "10 1074.5158382483216\n",
      "20 953.060725938025\n",
      "30 952.841803156986\n",
      "40 952.6366657505607\n",
      "50 952.4416793428418\n",
      "60 952.2560282226215\n",
      "70 952.0789616067855\n",
      "80 951.9097871726783\n",
      "90 951.7478649229877\n",
      "100 951.5926017230138\n",
      "110 951.4434464219922\n",
      "120 951.2998854817176\n",
      "130 951.1614390464965\n",
      "140 951.0276573975583\n",
      "150 950.8981177426882\n",
      "160 950.7724212983219\n",
      "170 950.6501906267921\n",
      "180 950.5310671960247\n",
      "190 950.4147091328983\n",
      "200 950.3007891447975\n",
      "210 950.1889925866717\n",
      "220 950.0790156533302\n",
      "230 949.9705636786827\n",
      "240 949.8633495253752\n",
      "250 949.7570920496796\n",
      "260 949.6515146277336\n",
      "270 949.546343730213\n",
      "280 949.44130753337\n",
      "290 949.3361345550478\n",
      "300 949.230552304821\n",
      "310 949.1242859378456\n",
      "320 949.0170569023162\n",
      "330 948.9085815706305\n",
      "340 948.7985698445115\n",
      "350 948.6867237243467\n",
      "360 948.5727358329914\n",
      "370 948.4562878841443\n",
      "380 948.3370490852311\n",
      "390 948.2146744644276\n",
      "400 948.088803111154\n",
      "410 947.9590563189001\n",
      "420 947.8250356187365\n",
      "430 947.6863206912514\n",
      "440 947.5424671438955\n",
      "450 947.3930041398797\n",
      "460 947.2374318637196\n",
      "470 947.0752188073362\n",
      "480 946.9057988592076\n",
      "490 946.7285681773985\n",
      "500 946.5428818253513\n",
      "510 946.3480501470344\n",
      "520 946.1433348553945\n",
      "530 945.9279448049725\n",
      "540 945.7010314160042\n",
      "550 945.4616837132924\n",
      "560 945.208922938617\n",
      "570 944.9416966903638\n",
      "580 944.6588725385798\n",
      "590 944.3592310576748\n",
      "600 944.0414582127722\n",
      "610 943.7041370292318\n",
      "620 943.3457384684268\n",
      "630 942.9646114265906\n",
      "640 942.5589717676824\n",
      "650 942.1268902960245\n",
      "660 941.6662795700918\n",
      "670 941.1748794554389\n",
      "680 940.6502413123931\n",
      "690 940.0897107126477\n",
      "700 939.4904085779755\n",
      "710 938.8492106332635\n",
      "720 938.1627250640407\n",
      "730 937.4272682641664\n",
      "740 936.6388385506318\n",
      "750 935.7930877071001\n",
      "760 934.8852901932005\n",
      "770 933.9103098197534\n",
      "780 932.8625636379534\n",
      "790 931.7359827208355\n",
      "800 930.5239694269922\n",
      "810 929.2193506311871\n",
      "820 927.8143262903153\n",
      "830 926.3004125987518\n",
      "840 924.6683788969765\n",
      "850 922.908177466582\n",
      "860 921.0088654251912\n",
      "870 918.9585181977156\n",
      "880 916.7441345788407\n",
      "890 914.3515343296453\n",
      "900 911.7652506987819\n",
      "910 908.9684223582295\n",
      "920 905.9426921065311\n",
      "930 902.6681233673227\n",
      "940 899.1231499240608\n",
      "950 895.2845792071387\n",
      "960 891.1276742217615\n",
      "970 886.6263429546984\n",
      "980 881.753465563375\n",
      "990 876.4813873816627\n",
      "1000 870.7825984757062\n",
      "1010 864.6306074820495\n",
      "1020 858.0009991337718\n",
      "1030 850.872642705624\n",
      "1040 843.2289947391052\n",
      "1050 835.0594161189036\n",
      "1060 826.3604034087583\n",
      "1070 817.1366214277301\n",
      "1080 807.4016253168957\n",
      "1090 797.1781841749696\n",
      "1100 786.4981686540923\n",
      "1110 775.4020337601387\n",
      "1120 763.9379936116738\n",
      "1130 752.1610196401912\n",
      "1140 740.1317802986867\n",
      "1150 727.9155836954136\n",
      "1160 715.5813112201239\n",
      "1170 703.2002747867084\n",
      "1180 690.8449178862516\n",
      "1190 678.5873156629984\n",
      "1200 666.4974956152093\n",
      "1210 654.6416711174232\n",
      "1220 643.0805290875353\n",
      "1230 631.8677262285271\n",
      "1240 621.0487251903158\n",
      "1250 610.6600539226224\n",
      "1260 600.7290144602015\n",
      "1270 591.2738157763179\n",
      "1280 582.3040679889829\n",
      "1290 573.8215547239888\n",
      "1300 565.8211948441507\n",
      "1310 558.2921098380015\n",
      "1320 551.2187244845486\n",
      "1330 544.5818424297717\n",
      "1340 538.3596527011049\n",
      "1350 532.5286366674173\n",
      "1360 527.0643569232996\n",
      "1370 521.9421197844482\n",
      "1380 517.1375114151881\n",
      "1390 512.6268140274647\n",
      "1400 508.38731312233375\n",
      "1410 504.39750951454096\n",
      "1420 500.6372511123992\n",
      "1430 497.08779941971454\n",
      "1440 493.73184481954956\n",
      "1450 490.55348321712364\n",
      "1460 487.5381648431448\n",
      "1470 484.672624168867\n",
      "1480 481.9447981132584\n",
      "1490 479.34373812425497\n",
      "1500 476.8595203360739\n",
      "1510 474.48315685474716\n",
      "1520 472.2065102944127\n",
      "1530 470.0222129551026\n",
      "1540 467.92359147118003\n",
      "1550 465.9045973395751\n",
      "1560 463.95974343173344\n",
      "1570 462.08404637885474\n",
      "1580 460.27297457620847\n",
      "1590 458.52240146217594\n",
      "1600 456.82856367748514\n",
      "1610 455.1880236889305\n",
      "1620 453.5976364611975\n",
      "1630 452.05451977362446\n",
      "1640 450.55602780087423\n",
      "1650 449.09972760384926\n",
      "1660 447.68337820707825\n",
      "1670 446.3049119693562\n",
      "1680 444.96241798430094\n",
      "1690 443.6541272759092\n",
      "1700 442.3783995806238\n",
      "1710 441.13371153159665\n",
      "1720 439.9186460827083\n",
      "1730 438.731883029458\n",
      "1740 437.5721905012243\n",
      "1750 436.43841731473657\n",
      "1760 435.32948609207733\n",
      "1770 434.24438705835837\n",
      "1780 433.1821724445042\n",
      "1790 432.14195142959585\n",
      "1800 431.1228855650479\n",
      "1810 430.1241846297256\n",
      "1820 429.1451028710518\n",
      "1830 428.184935592336\n",
      "1840 427.24301605108235\n",
      "1850 426.3187126369868\n",
      "1860 425.4114263017913\n",
      "1870 424.52058821620096\n",
      "1880 423.645657631733\n",
      "1890 422.7861199277182\n",
      "1900 421.9414848257452\n",
      "1910 421.111284755672\n",
      "1920 420.29507335895494\n",
      "1930 419.4924241164825\n",
      "1940 418.7029290893904\n",
      "1950 417.926197762478\n",
      "1960 417.1618559808601\n",
      "1970 416.4095449714135\n",
      "1980 415.6689204413852\n",
      "1990 414.93965174726986\n",
      "2000 414.2214211277167\n",
      "2010 413.51392299482933\n",
      "2020 412.8168632787334\n",
      "2030 412.12995882079724\n",
      "2040 411.45293681129\n",
      "2050 410.785534267679\n",
      "2060 410.1274975500985\n",
      "2070 409.47858191085794\n",
      "2080 408.83855107512613\n",
      "2090 408.2071768502041\n",
      "2100 407.58423876101904\n",
      "2110 406.9695237096946\n",
      "2120 406.3628256572353\n",
      "2130 405.76394532554417\n",
      "2140 405.1726899181424\n",
      "2150 404.588872858107\n",
      "2160 404.0123135418703\n",
      "2170 403.4428371076379\n",
      "2180 402.88027421729385\n",
      "2190 402.3244608507502\n",
      "2200 401.7752381117967\n",
      "2210 401.23245204456816\n",
      "2220 400.6959534598383\n",
      "2230 400.16559777040067\n",
      "2240 399.64124483486137\n",
      "2250 399.12275880922436\n",
      "2260 398.61000800569565\n",
      "2270 398.1028647581819\n",
      "2280 397.6012052939993\n",
      "2290 397.1049096113426\n",
      "2300 396.6138613621041\n",
      "2310 396.1279477396646\n",
      "2320 395.6470593713\n",
      "2330 395.1710902148833\n",
      "2340 394.69993745958374\n",
      "2350 394.2335014302823\n",
      "2360 393.77168549545394\n",
      "2370 393.314395978279\n",
      "2380 392.8615420707665\n",
      "2390 392.4130357506922\n",
      "2400 391.96879170116483\n",
      "2410 391.528727232654\n",
      "2420 391.0927622073227\n",
      "2430 390.66081896552436\n",
      "2440 390.2328222543323\n",
      "2450 389.8086991579845\n",
      "2460 389.3883790301347\n",
      "2470 388.97179342781163\n",
      "2480 388.5588760469985\n",
      "2490 388.1495626597511\n",
      "2500 387.74379105278285\n",
      "2510 387.3415009674528\n",
      "2520 386.94263404109887\n",
      "2530 386.54713374966406\n",
      "2540 386.1549453515736\n",
      "2550 385.766015832819\n",
      "2560 385.38029385322056\n",
      "2570 384.9977296938331\n",
      "2580 384.61827520547445\n",
      "2590 384.24188375835143\n",
      "2600 383.86851019276736\n",
      "2610 383.49811077089555\n",
      "2620 383.13064312960785\n",
      "2630 382.76606623434384\n",
      "2640 382.40434033402033\n",
      "2650 382.04542691696565\n",
      "2660 381.6892886678824\n",
      "2670 381.3358894258282\n",
      "2680 380.9851941432149\n",
      "2690 380.63716884582254\n",
      "2700 380.2917805938235\n",
      "2710 379.94899744381985\n",
      "2720 379.6087884118845\n",
      "2730 379.2711234376088\n",
      "2740 378.93597334914864\n",
      "2750 378.6033098292698\n",
      "2760 378.273105382382\n",
      "2770 377.9453333025604\n",
      "2780 377.61996764254593\n",
      "2790 377.29698318371476\n",
      "2800 376.9763554070128\n",
      "2810 376.65806046483885\n",
      "2820 376.34207515387095\n",
      "2830 376.02837688881834\n",
      "2840 375.71694367709046\n",
      "2850 375.40775409436384\n",
      "2860 375.100787261034\n",
      "2870 374.7960228195353\n",
      "2880 374.49344091250816\n",
      "2890 374.19302216180114\n",
      "2900 373.8947476482796\n",
      "2910 373.5985988924307\n",
      "2920 373.3045578357343\n",
      "2930 373.0126068227866\n",
      "2940 372.7227285841484\n",
      "2950 372.43490621989923\n",
      "2960 372.14912318387314\n",
      "2970 371.86536326855406\n",
      "2980 371.58361059060684\n",
      "2990 371.30384957702245\n",
      "3000 371.026064951851\n",
      "3010 370.75024172350646\n",
      "3020 370.47636517261026\n",
      "3030 370.20442084036097\n",
      "3040 369.9343945174027\n",
      "3050 369.66627223317096\n",
      "3060 369.4000402456956\n",
      "3070 369.13568503184035\n",
      "3080 368.87319327795694\n",
      "3090 368.61255187093394\n",
      "3100 368.3537478896246\n",
      "3110 368.0967685966314\n",
      "3120 367.8416014304295\n",
      "3130 367.58823399781454\n",
      "3140 367.33665406665716\n",
      "3150 367.0868495589451\n",
      "3160 366.8388085441053\n",
      "3170 366.59251923258137\n",
      "3180 366.34796996966287\n",
      "3190 366.1051492295473\n",
      "3200 365.86404560962256\n",
      "3210 365.6246478249622\n",
      "3220 365.3869447030197\n",
      "3230 365.15092517850997\n",
      "3240 364.9165782884733\n",
      "3250 364.6838931675095\n",
      "3260 364.45285904317444\n",
      "3270 364.2234652315318\n",
      "3280 363.99570113285193\n",
      "3290 363.769556227455\n",
      "3300 363.5450200716869\n",
      "3310 363.3220822940276\n",
      "3320 363.1007325913242\n",
      "3330 362.88096072514384\n",
      "3340 362.6627565182471\n",
      "3350 362.4461098511722\n",
      "3360 362.23101065893115\n",
      "3370 362.0174489278139\n",
      "3380 361.80541469229865\n",
      "3390 361.5948980320651\n",
      "3400 361.385889069111\n",
      "3410 361.17837796496934\n",
      "3420 360.97235491802394\n",
      "3430 360.767810160927\n",
      "3440 360.5647339581128\n",
      "3450 360.3631166034099\n",
      "3460 360.16294841775203\n",
      "3470 359.9642197469863\n",
      "3480 359.76692095977825\n",
      "3490 359.5710424456175\n",
      "3500 359.37657461291917\n",
      "3510 359.1835078872249\n",
      "3520 358.9918327095039\n",
      "3530 358.8015395345513\n",
      "3540 358.61261882948855\n",
      "3550 358.42506107236215\n",
      "3560 358.238856750845\n",
      "3570 358.0539963610359\n",
      "3580 357.8704704063636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3590 357.68826939658857\n",
      "3600 357.50738384690953\n",
      "3610 357.3278042771678\n",
      "3620 357.1495212111579\n",
      "3630 356.9725251760324\n",
      "3640 356.796806701813\n",
      "3650 356.6223563209994\n",
      "3660 356.44916456827605\n",
      "3670 356.27722198032046\n",
      "3680 356.1065190957054\n",
      "3690 355.93704645489896\n",
      "3700 355.76879460035923\n",
      "3710 355.60175407672193\n",
      "3720 355.43591543107846\n",
      "3730 355.27126921334434\n",
      "3740 355.1078059767145\n",
      "3750 354.9455162782045\n",
      "3760 354.7843906792715\n",
      "3770 354.6244197465186\n",
      "3780 354.4655940524742\n",
      "3790 354.3079041764462\n",
      "3800 354.1513407054469\n",
      "3810 353.99589423518665\n",
      "3820 353.8415553711319\n",
      "3830 353.6883147296233\n",
      "3840 353.5361629390529\n",
      "3850 353.3850906410954\n",
      "3860 353.2350884919882\n",
      "3870 353.086147163859\n",
      "3880 352.9382573460966\n",
      "3890 352.791409746759\n",
      "3900 352.64559509401784\n",
      "3910 352.5008041376324\n",
      "3920 352.3570276504519\n",
      "3930 352.21425642993967\n",
      "3940 352.0724812997166\n",
      "3950 351.93169311112075\n",
      "3960 351.7918827447773\n",
      "3970 351.6530411121761\n",
      "3980 351.5151591572532\n",
      "3990 351.37822785797283\n",
      "4000 351.2422382279053\n",
      "4010 351.10718131779845\n",
      "4020 350.9730482171386\n",
      "4030 350.83983005569735\n",
      "4040 350.70751800506287\n",
      "4050 350.5761032801506\n",
      "4060 350.44557714069026\n",
      "4070 350.3159308926899\n",
      "4080 350.18715588986896\n",
      "4090 350.05924353506356\n",
      "4100 349.93218528159696\n",
      "4110 349.805972634615\n",
      "4120 349.68059715238616\n",
      "4130 349.55605044756044\n",
      "4140 349.43232418838926\n",
      "4150 349.30941009990255\n",
      "4160 349.1872999650415\n",
      "4170 349.06598562574817\n",
      "4180 348.94545898400713\n",
      "4190 348.8257120028415\n",
      "4200 348.7067367072608\n",
      "4210 348.5885251851608\n",
      "4220 348.47106958817335\n",
      "4230 348.3543621324691\n",
      "4240 348.2383950995078\n",
      "4250 348.12316083674096\n",
      "4260 348.0086517582655\n",
      "4270 347.8948603454245\n",
      "4280 347.7817791473628\n",
      "4290 347.66940078153044\n",
      "4300 347.5577179341385\n",
      "4310 347.44672336056874\n",
      "4320 347.3364098857321\n",
      "4330 347.2267704043836\n",
      "4340 347.11779788138966\n",
      "4350 347.0094853519521\n",
      "4360 346.9018259217846\n",
      "4370 346.7948127672491\n",
      "4380 346.68843913544896\n",
      "4390 346.5826983442805\n",
      "4400 346.47758378244487\n",
      "4410 346.37308890942063\n",
      "4420 346.2692072553988\n",
      "4430 346.1659324211823\n",
      "4440 346.06325807804893\n",
      "4450 345.9611779675799\n",
      "4460 345.85968590145745\n",
      "4470 345.75877576122946\n"
     ]
    }
   ],
   "source": [
    "hidden_lay = [5, 5]\n",
    "lw, lb = neural_train(train_inp, train_out, hidden_lay, 0.001, len(inp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8921052631578947\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "with open(\"toy_data/toy_testX.csv\", \"r\") as f_obj:\n",
    "    inp_test = csv_reader(f_obj)\n",
    "with open(\"toy_data/toy_testY.csv\", \"r\") as f_obj:\n",
    "    out_exp_test = csv_reader_y(f_obj)\n",
    "\n",
    "\n",
    "    \n",
    "corr = 0\n",
    "total = 0\n",
    "for data_pt, corr_pred in zip(train_inp, train_out):\n",
    "#     print(corr_pred[0], neural_predict(data_pt))\n",
    "    if(corr_pred == neural_predict(data_pt)):\n",
    "        corr += 1\n",
    "    total += 1\n",
    "\n",
    "print((corr+0.0)/total)\n",
    "\n",
    "corr = 0\n",
    "total = 0\n",
    "for data_pt, corr_pred in zip(inp_test, out_exp_test):\n",
    "#     print(corr_pred[0], neural_predict(data_pt))\n",
    "    if(corr_pred == neural_predict(data_pt)):\n",
    "        corr += 1\n",
    "    total += 1\n",
    "\n",
    "print((corr+0.0)/total)\n",
    "\n",
    "# loglin = linear_model.LogisticRegression()\n",
    "\n",
    "# loglin.fit(train_inp, train_out)\n",
    "\n",
    "# total = 0\n",
    "# corr = 0\n",
    "# one = 0\n",
    "# zero = 0\n",
    "# ans = loglin.predict(inp_test)\n",
    "# print(metrics.accuracy_score(ans, out_exp_test))\n",
    "# ans = loglin.predict(train_inp)\n",
    "# print(metrics.accuracy_score(ans, train_out))\n",
    "\n",
    "# plot_decision_boundary(lambda x:loglin.predict(x), np.array(inp_test), np.array(out_exp_test), \"lin_reg_test.png\")\n",
    "# plot_decision_boundary(lambda x:loglin.predict(x), np.array(train_inp), np.array(train_out), \"lin_reg_train.png\")\n",
    "\n",
    "plot_decision_boundary(lambda x:neural_predict(x), np.array(inp_test), np.array(out_exp_test), \"neural_test_5_5.png\")\n",
    "# plot_decision_boundary(lambda x:neural_predict(x), np.array(train_inp), np.array(train_out), \"neural_train_5.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_data/MNIST_train.csv\", \"r\") as f_obj:\n",
    "    train_inp, train_out = mnist_reader(f_obj)\n",
    "with open(\"mnist_data/MNIST_test.csv\", \"r\") as f_obj:\n",
    "    inp_test, out_exp_test = mnist_reader(f_obj)\n",
    "# print(train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Epoch 0\n",
      "0 0.0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\prakhar ganesh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "0\n",
      "Epoch 2\n",
      "0\n",
      "Epoch 3\n",
      "0\n",
      "Epoch 4\n",
      "0\n",
      "Epoch 5\n",
      "0\n",
      "Epoch 6\n",
      "0\n",
      "Epoch 7\n",
      "0\n",
      "Epoch 8\n",
      "0\n",
      "Epoch 9\n",
      "0\n",
      "Epoch 10\n",
      "10 602.4707143471488\n",
      "0\n",
      "Epoch 11\n",
      "0\n",
      "Epoch 12\n",
      "0\n",
      "Epoch 13\n",
      "0\n",
      "Epoch 14\n",
      "0\n",
      "Epoch 15\n",
      "0\n",
      "Epoch 16\n",
      "0\n",
      "Epoch 17\n",
      "0\n",
      "Epoch 18\n",
      "0\n",
      "Epoch 19\n",
      "0\n",
      "Epoch 20\n",
      "20 385.64715063354265\n",
      "0\n",
      "Epoch 21\n",
      "0\n",
      "Epoch 22\n",
      "0\n",
      "Epoch 23\n",
      "0\n",
      "Epoch 24\n",
      "0\n",
      "Epoch 25\n",
      "0\n",
      "Epoch 26\n",
      "0\n",
      "Epoch 27\n",
      "0\n",
      "Epoch 28\n",
      "0\n",
      "Epoch 29\n",
      "0\n",
      "Epoch 30\n",
      "30 309.3546161046396\n",
      "0\n",
      "Epoch 31\n",
      "0\n",
      "Epoch 32\n",
      "0\n",
      "Epoch 33\n",
      "0\n",
      "Epoch 34\n",
      "0\n",
      "Epoch 35\n",
      "0\n",
      "Epoch 36\n",
      "0\n",
      "Epoch 37\n",
      "0\n",
      "Epoch 38\n",
      "0\n",
      "Epoch 39\n",
      "0\n",
      "Epoch 40\n",
      "40 272.23397227466313\n",
      "0\n",
      "Epoch 41\n",
      "0\n",
      "Epoch 42\n",
      "0\n",
      "Epoch 43\n",
      "0\n",
      "Epoch 44\n",
      "0\n",
      "Epoch 45\n",
      "0\n",
      "Epoch 46\n",
      "0\n",
      "Epoch 47\n",
      "0\n",
      "Epoch 48\n",
      "0\n",
      "Epoch 49\n",
      "0\n",
      "Epoch 50\n",
      "50 245.9028848192191\n",
      "0\n",
      "Epoch 51\n",
      "0\n",
      "Epoch 52\n",
      "0\n",
      "Epoch 53\n",
      "0\n",
      "Epoch 54\n",
      "0\n",
      "Epoch 55\n",
      "0\n",
      "Epoch 56\n",
      "0\n",
      "Epoch 57\n",
      "0\n",
      "Epoch 58\n",
      "0\n",
      "Epoch 59\n",
      "0\n",
      "Epoch 60\n",
      "60 216.24673310734693\n",
      "0\n",
      "Epoch 61\n",
      "0\n",
      "Epoch 62\n",
      "0\n",
      "Epoch 63\n",
      "0\n",
      "Epoch 64\n",
      "0\n",
      "Epoch 65\n",
      "0\n",
      "Epoch 66\n",
      "0\n",
      "Epoch 67\n",
      "0\n",
      "Epoch 68\n",
      "0\n",
      "Epoch 69\n",
      "0\n",
      "Epoch 70\n",
      "70 188.8133836599242\n",
      "0\n",
      "Epoch 71\n",
      "0\n",
      "Epoch 72\n",
      "0\n",
      "Epoch 73\n",
      "0\n",
      "Epoch 74\n",
      "0\n",
      "Epoch 75\n",
      "0\n",
      "Epoch 76\n",
      "0\n",
      "Epoch 77\n",
      "0\n",
      "Epoch 78\n",
      "0\n",
      "Epoch 79\n",
      "0\n",
      "Epoch 80\n",
      "80 167.71228394186573\n",
      "0\n",
      "Epoch 81\n",
      "0\n",
      "Epoch 82\n",
      "0\n",
      "Epoch 83\n",
      "0\n",
      "Epoch 84\n",
      "0\n",
      "Epoch 85\n",
      "0\n",
      "Epoch 86\n",
      "0\n",
      "Epoch 87\n",
      "0\n",
      "Epoch 88\n",
      "0\n",
      "Epoch 89\n",
      "0\n",
      "Epoch 90\n",
      "90 152.0607665869025\n",
      "0\n",
      "Epoch 91\n",
      "0\n",
      "Epoch 92\n",
      "0\n",
      "Epoch 93\n",
      "0\n",
      "Epoch 94\n",
      "0\n",
      "Epoch 95\n",
      "0\n",
      "Epoch 96\n",
      "0\n",
      "Epoch 97\n",
      "0\n",
      "Epoch 98\n",
      "0\n",
      "Epoch 99\n",
      "0\n",
      "Epoch 100\n",
      "100 140.23235527964957\n",
      "0\n",
      "Epoch 101\n",
      "0\n",
      "Epoch 102\n",
      "0\n",
      "Epoch 103\n",
      "0\n",
      "Epoch 104\n",
      "0\n",
      "Epoch 105\n",
      "0\n",
      "Epoch 106\n",
      "0\n",
      "Epoch 107\n",
      "0\n",
      "Epoch 108\n",
      "0\n",
      "Epoch 109\n",
      "0\n",
      "Epoch 110\n",
      "110 130.8562519550393\n",
      "0\n",
      "Epoch 111\n",
      "0\n",
      "Epoch 112\n",
      "0\n",
      "Epoch 113\n",
      "0\n",
      "Epoch 114\n",
      "0\n",
      "Epoch 115\n",
      "0\n",
      "Epoch 116\n",
      "0\n",
      "Epoch 117\n",
      "0\n",
      "Epoch 118\n",
      "0\n",
      "Epoch 119\n",
      "0\n",
      "Epoch 120\n",
      "120 123.03951713880402\n",
      "0\n",
      "Epoch 121\n",
      "0\n",
      "Epoch 122\n",
      "0\n",
      "Epoch 123\n",
      "0\n",
      "Epoch 124\n",
      "0\n",
      "Epoch 125\n",
      "0\n",
      "Epoch 126\n",
      "0\n",
      "Epoch 127\n",
      "0\n",
      "Epoch 128\n",
      "0\n",
      "Epoch 129\n",
      "0\n",
      "Epoch 130\n",
      "130 116.29460511697032\n",
      "0\n",
      "Epoch 131\n",
      "0\n",
      "Epoch 132\n",
      "0\n",
      "Epoch 133\n",
      "0\n",
      "Epoch 134\n",
      "0\n",
      "Epoch 135\n",
      "0\n",
      "Epoch 136\n",
      "0\n",
      "Epoch 137\n",
      "0\n",
      "Epoch 138\n",
      "0\n",
      "Epoch 139\n",
      "0\n",
      "Epoch 140\n",
      "140 110.3673052982391\n",
      "0\n",
      "Epoch 141\n",
      "0\n",
      "Epoch 142\n",
      "0\n",
      "Epoch 143\n",
      "0\n",
      "Epoch 144\n",
      "0\n",
      "Epoch 145\n",
      "0\n",
      "Epoch 146\n",
      "0\n",
      "Epoch 147\n",
      "0\n",
      "Epoch 148\n",
      "0\n",
      "Epoch 149\n",
      "0\n",
      "Epoch 150\n",
      "150 105.12610369509459\n",
      "0\n",
      "Epoch 151\n",
      "0\n",
      "Epoch 152\n",
      "0\n",
      "Epoch 153\n",
      "0\n",
      "Epoch 154\n",
      "0\n",
      "Epoch 155\n",
      "0\n",
      "Epoch 156\n",
      "0\n",
      "Epoch 157\n",
      "0\n",
      "Epoch 158\n",
      "0\n",
      "Epoch 159\n",
      "0\n",
      "Epoch 160\n",
      "160 100.50154601645075\n",
      "0\n",
      "Epoch 161\n",
      "0\n",
      "Epoch 162\n",
      "0\n",
      "Epoch 163\n",
      "0\n",
      "Epoch 164\n",
      "0\n",
      "Epoch 165\n",
      "0\n",
      "Epoch 166\n",
      "0\n",
      "Epoch 167\n",
      "0\n",
      "Epoch 168\n",
      "0\n",
      "Epoch 169\n",
      "0\n",
      "Epoch 170\n",
      "170 96.44396808375069\n",
      "0\n",
      "Epoch 171\n",
      "0\n",
      "Epoch 172\n",
      "0\n",
      "Epoch 173\n",
      "0\n",
      "Epoch 174\n",
      "0\n",
      "Epoch 175\n",
      "0\n",
      "Epoch 176\n",
      "0\n",
      "Epoch 177\n",
      "0\n",
      "Epoch 178\n",
      "0\n",
      "Epoch 179\n",
      "0\n",
      "Epoch 180\n",
      "180 92.89831555886454\n",
      "0\n",
      "Epoch 181\n",
      "0\n",
      "Epoch 182\n",
      "0\n",
      "Epoch 183\n",
      "0\n",
      "Epoch 184\n",
      "0\n",
      "Epoch 185\n",
      "0\n",
      "Epoch 186\n",
      "0\n",
      "Epoch 187\n",
      "0\n",
      "Epoch 188\n",
      "0\n",
      "Epoch 189\n",
      "0\n",
      "Epoch 190\n",
      "190 89.79765128586698\n",
      "0\n",
      "Epoch 191\n",
      "0\n",
      "Epoch 192\n",
      "0\n",
      "Epoch 193\n",
      "0\n",
      "Epoch 194\n",
      "0\n",
      "Epoch 195\n",
      "0\n",
      "Epoch 196\n",
      "0\n",
      "Epoch 197\n",
      "0\n",
      "Epoch 198\n",
      "0\n",
      "Epoch 199\n",
      "0\n",
      "Epoch 200\n",
      "200 87.06842340698906\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "hidden_lay = [100]\n",
    "print(len(train_inp))\n",
    "lw, lb = neural_train(train_inp[1750:1850], train_out[1750:1850], hidden_lay, 0.001, 100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\prakhar ganesh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1936\n",
      "0.5377777777777778\n"
     ]
    }
   ],
   "source": [
    "corr = 0\n",
    "total = 0\n",
    "for data_pt, corr_pred in zip(inp_test, out_exp_test):\n",
    "#     print(neural_predict(data_pt))\n",
    "    if(corr_pred == neural_predict(data_pt)):\n",
    "        corr += 1\n",
    "    total += 1\n",
    "\n",
    "print(corr)\n",
    "print((corr+0.0)/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
